{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "certain-domain",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import gym\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from numpy.random import default_rng\n",
    "rng = default_rng()\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Transition:\n",
    "    pass\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class CartpoleTransition(Transition):\n",
    "    s: np.ndarray\n",
    "    s_prime: np.ndarray\n",
    "    a: int\n",
    "    r: float\n",
    "    is_done: bool\n",
    "        \n",
    "class ReplayMemory():\n",
    "    def __init__(self,buffer_size):\n",
    "        self.buffer = deque([],maxlen=buffer_size)\n",
    "        \n",
    "    def append(self,transition):\n",
    "        self.buffer.appendleft(transition)\n",
    "    \n",
    "    def sample(self,batch_size=1):\n",
    "        return random.sample(self.buffer,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "found-walker",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateTransitionModel(nn.Module):\n",
    "    def __init__(self,feature_cnt,action_cnt):\n",
    "        super(StateTransitionModel, self).__init__()\n",
    "        \n",
    "        hidden_layer_cnt = 64\n",
    "        hidden_layer2_cnt = 64\n",
    "        \n",
    "        self.fc1 = nn.Linear(feature_cnt,hidden_layer_cnt)\n",
    "        #self.fc2 = nn.Linear(hidden_layer_cnt,hidden_layer2_cnt)\n",
    "        self.fc3 = nn.Linear(hidden_layer2_cnt,action_cnt)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        #x = torch.tanh(self.fc2(x))\n",
    "        #x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "clinical-germany",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,**args):\n",
    "        \n",
    "        print(args)\n",
    "        \n",
    "        self.action_cnt = args['action_cnt'] #env.action_space.n\n",
    "        self.feature_cnt = args['feature_cnt'] #env.observation_space.shape[0]\n",
    "        \n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.smoothing = args['smoothing'] if 'smoothing' in args else 0.5\n",
    "        \n",
    "        self.target_transition_network = StateTransitionModel(self.feature_cnt,self.action_cnt).to(self.device)\n",
    "        self.base_transition_network = StateTransitionModel(self.feature_cnt,self.action_cnt).to(self.device)\n",
    "        self.lr = args['lr'] if 'lr' in args else 0.01\n",
    "        self.optimizer = optim.Adam(self.target_transition_network.parameters(),lr=self.lr)\n",
    "        \n",
    "        self.alpha = args['alpha'] if 'alpha' in args else 1.0\n",
    "        self.gamma = args['gamma'] if 'gamma' in args else 1.0\n",
    "        self.batch_size = args['batch_size'] if 'batch_size' in args else 20\n",
    "        self.memory_size = args['memory_size'] if 'memory_size' in args else 200\n",
    "        \n",
    "        self.memory = deque([],maxlen=self.memory_size)\n",
    "        \n",
    "        self.epsilon = args['epsilon'] if 'epsilon' in args else 1.0\n",
    "        self.epsilon_decay = args['epsilon_decay'] if 'epsilon_decay' in args else 0.99\n",
    "        self.epsilon_min = args['epsilon_min'] if 'epsilon_min' in args else 0.1\n",
    "        \n",
    "    \n",
    "    def _update_base_network(self):\n",
    "        \"\"\"\n",
    "        Double DQN uses a smoothing factor when updating base transition model. \n",
    "        Smoothing improves the model's training stability by 'smoothing' large variations in the target model weight update values  \n",
    "        \"\"\"\n",
    "        \n",
    "        base_params = self.base_transition_network.named_parameters()\n",
    "        target_params = self.target_transition_network.named_parameters()\n",
    "        \n",
    "        blended_weights = {}\n",
    "        for base_param,target_param in zip(base_params,target_params):\n",
    "            blended_weights[base_param[0]] = self.smoothing*base_param[1].data+(1-self.smoothing)*target_param[1].data\n",
    "        \n",
    "        self.base_transition_network.load_state_dict(blended_weights)\n",
    "    \n",
    "    def _update_epsilon(self):\n",
    "        if self.epsilon >self.epsilon_min:\n",
    "            self.epsilon = max(self.epsilon_min,self.epsilon*self.epsilon_decay)       \n",
    "        \n",
    "        \n",
    "    def get_action(self,s,use_epsilon_decay=True):\n",
    "        if np.random.random() < self.epsilon and use_epsilon_decay:\n",
    "            action = random.randint(0,self.action_cnt-1)\n",
    "        else:\n",
    "            action = torch.argmax(self.base_transition_network.forward(torch.from_numpy(s.astype(np.float32)).to(self.device))).item()\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def append_replay(self,transition):\n",
    "        self.memory.appendleft(transition)\n",
    "    \n",
    "    \n",
    "    def update_transition_model(self):\n",
    "\n",
    "        if len(self.memory)<self.batch_size:\n",
    "            return\n",
    "        \n",
    "        batch_cnt=10\n",
    "        for _ in range(batch_cnt):\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            minibatch_ix = rng.choice(len(self.memory), size=self.batch_size, replace=False)\n",
    "\n",
    "            state = torch.from_numpy(np.array([self.memory[v]['s'] for v in minibatch_ix]).astype(np.float32)).to(self.device)\n",
    "            next_state = torch.from_numpy(np.array([self.memory[v]['s_prime'] for v in minibatch_ix]).astype(np.float32)).to(self.device)\n",
    "            reward = np.array([self.memory[v]['r'] for v in minibatch_ix])\n",
    "            action_ix = np.array([self.memory[v]['a'] for v in minibatch_ix])\n",
    "\n",
    "            target = self.base_transition_network.forward(state)\n",
    "            for i in range(len(action_ix)):\n",
    "                target[i,action_ix[i]]=reward[i]\n",
    "\n",
    "            future_transition = self.base_transition_network.forward(next_state)\n",
    "            has_future_states = torch.reshape(torch.from_numpy(np.array([not self.memory[v]['is_done'] for v in minibatch_ix])),(-1,1)).to(self.device)\n",
    "            discounted_reward = torch.max(future_transition*has_future_states,axis=1).values*self.gamma\n",
    "\n",
    "            for i in range(len(action_ix)):\n",
    "                target[i,action_ix[i]]+=discounted_reward[i]\n",
    "\n",
    "            # Backprop Target Network\n",
    "            prediction = self.target_transition_network.forward(state)            \n",
    "            loss = self.criterion(prediction,target)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        self._update_epsilon()\n",
    "        self._update_base_network()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "progressive-scientist",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulator():\n",
    "    pass\n",
    "\n",
    "class CartpoleSimulator(Simulator):\n",
    "    def __init__(self,agent:Agent,**args):\n",
    "        self.env = gym.make(\"CartPole-v1\")\n",
    "        self.agent = agent\n",
    "    \n",
    "    def terminate(self):\n",
    "        self.env.close()\n",
    "    \n",
    "    def run_trial(self,use_epsilon=True):\n",
    "        \n",
    "        curr_state = self.env.reset()\n",
    "        replay_records = []\n",
    "        cum_reward=0\n",
    "        done=False\n",
    "        \n",
    "        while cum_reward<2000 and not done: \n",
    "            action = self.agent.get_action(curr_state,use_epsilon)\n",
    "            s_prime, r, done, _ = self.env.step(action)\n",
    "            \n",
    "            if done and cum_reward<499:\n",
    "                r = -100\n",
    "            \n",
    "            transition = {'s':curr_state,'s_prime':s_prime,'a':action,'r':r,'is_done':done}\n",
    "            self.agent.append_replay(transition)\n",
    "            cum_reward += 1\n",
    "            curr_state = s_prime\n",
    "            \n",
    "        self.rewards.append(cum_reward)\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        run_cnt=1000\n",
    "        self.rewards = []\n",
    "        trial_cnt=10\n",
    "        \n",
    "        while run_cnt>0:\n",
    "            \n",
    "            for trial in range(trial_cnt):\n",
    "                self.run_trial()\n",
    "            self.agent.update_transition_model()\n",
    "            \n",
    "            if run_cnt%10==0:\n",
    "                print(f'Experiment {1000-run_cnt}: reward={np.mean(self.rewards)}, epsilon={self.agent.epsilon}, memory={len(self.agent.memory)}')\n",
    "                self.rewards=[]\n",
    "            run_cnt-=1\n",
    "        \n",
    "    def test(self):\n",
    "        self.env.reset()\n",
    "        while True:\n",
    "            self.env.render()\n",
    "            self.run_trial(use_epsilon=False)\n",
    "        for _ in range(iteration):\n",
    "            self.step(visualize=True)\n",
    "        self.terminate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "favorite-sunday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action_cnt': 2, 'feature_cnt': 4, 'alpha': 1.0, 'gamma': 0.99999, 'memory_size': 10000, 'batch_size': 64, 'lr': 0.001, 'epsilon': 0.9, 'epsilon_decay': 0.99, 'epsilon_min': 0.2}\n"
     ]
    }
   ],
   "source": [
    "params = {'action_cnt':2,'feature_cnt':4,'alpha':1.0,'gamma':0.99999,'memory_size':10000,'batch_size':64,'lr':0.001, 'epsilon':0.90,'epsilon_decay':0.99,'epsilon_min':0.20}\n",
    "agent = Agent(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "quiet-oriental",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "varying-identifier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 0: reward=18.2, epsilon=0.891, memory=182\n",
      "Experiment 10: reward=19.29, epsilon=0.8058044288328449, memory=2111\n",
      "Experiment 20: reward=20.78, epsilon=0.7287550813991327, memory=4189\n",
      "Experiment 30: reward=22.11, epsilon=0.6590730326889578, memory=6400\n",
      "Experiment 40: reward=42.82, epsilon=0.5960538368855853, memory=10000\n",
      "Experiment 50: reward=40.97, epsilon=0.5390604058195451, memory=10000\n",
      "Experiment 60: reward=20.87, epsilon=0.48751656837016827, memory=10000\n",
      "Experiment 70: reward=19.72, epsilon=0.4409012457037845, memory=10000\n",
      "Experiment 80: reward=38.48, epsilon=0.39874318346355536, memory=10000\n",
      "Experiment 90: reward=41.31, epsilon=0.3606161876563865, memory=10000\n",
      "Experiment 100: reward=21.68, epsilon=0.3261348160744472, memory=10000\n",
      "Experiment 110: reward=21.29, epsilon=0.2949504816940234, memory=10000\n",
      "Experiment 120: reward=32.36, epsilon=0.26674792865928726, memory=10000\n",
      "Experiment 130: reward=83.98, epsilon=0.24124204522518672, memory=10000\n",
      "Experiment 140: reward=74.47, epsilon=0.2181749814401222, memory=10000\n",
      "Experiment 150: reward=64.91, epsilon=0.2, memory=10000\n",
      "Experiment 160: reward=65.82, epsilon=0.2, memory=10000\n",
      "Experiment 170: reward=63.78, epsilon=0.2, memory=10000\n",
      "Experiment 180: reward=65.29, epsilon=0.2, memory=10000\n",
      "Experiment 190: reward=51.88, epsilon=0.2, memory=10000\n",
      "Experiment 200: reward=58.42, epsilon=0.2, memory=10000\n",
      "Experiment 210: reward=58.53, epsilon=0.2, memory=10000\n",
      "Experiment 220: reward=94.26, epsilon=0.2, memory=10000\n",
      "Experiment 230: reward=103.62, epsilon=0.2, memory=10000\n",
      "Experiment 240: reward=105.3, epsilon=0.2, memory=10000\n",
      "Experiment 250: reward=100.92, epsilon=0.2, memory=10000\n",
      "Experiment 260: reward=121.86, epsilon=0.2, memory=10000\n",
      "Experiment 270: reward=140.08, epsilon=0.2, memory=10000\n",
      "Experiment 280: reward=108.46, epsilon=0.2, memory=10000\n",
      "Experiment 290: reward=112.07, epsilon=0.2, memory=10000\n",
      "Experiment 300: reward=113.49, epsilon=0.2, memory=10000\n",
      "Experiment 310: reward=86.55, epsilon=0.2, memory=10000\n",
      "Experiment 320: reward=89.03, epsilon=0.2, memory=10000\n",
      "Experiment 330: reward=83.05, epsilon=0.2, memory=10000\n",
      "Experiment 340: reward=78.43, epsilon=0.2, memory=10000\n",
      "Experiment 350: reward=111.94, epsilon=0.2, memory=10000\n",
      "Experiment 360: reward=106.44, epsilon=0.2, memory=10000\n",
      "Experiment 370: reward=118.6, epsilon=0.2, memory=10000\n",
      "Experiment 380: reward=110.69, epsilon=0.2, memory=10000\n",
      "Experiment 390: reward=110.74, epsilon=0.2, memory=10000\n",
      "Experiment 400: reward=101.72, epsilon=0.2, memory=10000\n",
      "Experiment 410: reward=106.4, epsilon=0.2, memory=10000\n",
      "Experiment 420: reward=110.23, epsilon=0.2, memory=10000\n",
      "Experiment 430: reward=104.27, epsilon=0.2, memory=10000\n",
      "Experiment 440: reward=119.62, epsilon=0.2, memory=10000\n",
      "Experiment 450: reward=117.23, epsilon=0.2, memory=10000\n",
      "Experiment 460: reward=107.02, epsilon=0.2, memory=10000\n",
      "Experiment 470: reward=83.13, epsilon=0.2, memory=10000\n",
      "Experiment 480: reward=65.19, epsilon=0.2, memory=10000\n",
      "Experiment 490: reward=52.44, epsilon=0.2, memory=10000\n",
      "Experiment 500: reward=47.27, epsilon=0.2, memory=10000\n",
      "Experiment 510: reward=54.29, epsilon=0.2, memory=10000\n",
      "Experiment 520: reward=59.34, epsilon=0.2, memory=10000\n",
      "Experiment 530: reward=98.74, epsilon=0.2, memory=10000\n",
      "Experiment 540: reward=105.83, epsilon=0.2, memory=10000\n",
      "Experiment 550: reward=126.27, epsilon=0.2, memory=10000\n",
      "Experiment 560: reward=134.52, epsilon=0.2, memory=10000\n",
      "Experiment 570: reward=162.88, epsilon=0.2, memory=10000\n",
      "Experiment 580: reward=223.73, epsilon=0.2, memory=10000\n",
      "Experiment 590: reward=167.55, epsilon=0.2, memory=10000\n",
      "Experiment 600: reward=178.23, epsilon=0.2, memory=10000\n",
      "Experiment 610: reward=133.69, epsilon=0.2, memory=10000\n",
      "Experiment 620: reward=227.81, epsilon=0.2, memory=10000\n",
      "Experiment 630: reward=146.85, epsilon=0.2, memory=10000\n",
      "Experiment 640: reward=227.37, epsilon=0.2, memory=10000\n",
      "Experiment 650: reward=198.87, epsilon=0.2, memory=10000\n",
      "Experiment 660: reward=244.36, epsilon=0.2, memory=10000\n",
      "Experiment 670: reward=298.18, epsilon=0.2, memory=10000\n",
      "Experiment 680: reward=274.16, epsilon=0.2, memory=10000\n",
      "Experiment 690: reward=334.56, epsilon=0.2, memory=10000\n",
      "Experiment 700: reward=373.43, epsilon=0.2, memory=10000\n",
      "Experiment 710: reward=422.74, epsilon=0.2, memory=10000\n",
      "Experiment 720: reward=482.94, epsilon=0.2, memory=10000\n",
      "Experiment 730: reward=492.2, epsilon=0.2, memory=10000\n",
      "Experiment 740: reward=478.65, epsilon=0.2, memory=10000\n",
      "Experiment 750: reward=481.79, epsilon=0.2, memory=10000\n",
      "Experiment 760: reward=478.41, epsilon=0.2, memory=10000\n",
      "Experiment 770: reward=458.68, epsilon=0.2, memory=10000\n",
      "Experiment 780: reward=426.16, epsilon=0.2, memory=10000\n",
      "Experiment 790: reward=225.41, epsilon=0.2, memory=10000\n",
      "Experiment 800: reward=62.81, epsilon=0.2, memory=10000\n",
      "Experiment 810: reward=29.14, epsilon=0.2, memory=10000\n",
      "Experiment 820: reward=16.79, epsilon=0.2, memory=10000\n",
      "Experiment 830: reward=14.95, epsilon=0.2, memory=10000\n",
      "Experiment 840: reward=76.72, epsilon=0.2, memory=10000\n",
      "Experiment 850: reward=411.43, epsilon=0.2, memory=10000\n",
      "Experiment 860: reward=482.87, epsilon=0.2, memory=10000\n",
      "Experiment 870: reward=477.74, epsilon=0.2, memory=10000\n",
      "Experiment 880: reward=477.45, epsilon=0.2, memory=10000\n",
      "Experiment 890: reward=480.85, epsilon=0.2, memory=10000\n",
      "Experiment 900: reward=470.91, epsilon=0.2, memory=10000\n",
      "Experiment 910: reward=445.2, epsilon=0.2, memory=10000\n",
      "Experiment 920: reward=452.46, epsilon=0.2, memory=10000\n",
      "Experiment 930: reward=451.17, epsilon=0.2, memory=10000\n",
      "Experiment 940: reward=428.75, epsilon=0.2, memory=10000\n",
      "Experiment 950: reward=416.62, epsilon=0.2, memory=10000\n",
      "Experiment 960: reward=451.11, epsilon=0.2, memory=10000\n",
      "Experiment 970: reward=431.18, epsilon=0.2, memory=10000\n",
      "Experiment 980: reward=438.06, epsilon=0.2, memory=10000\n",
      "Experiment 990: reward=437.28, epsilon=0.2, memory=10000\n"
     ]
    }
   ],
   "source": [
    "sim = CartpoleSimulator(agent,**params)\n",
    "sim.train()#visualize=True)\n",
    "#sim.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aware-presence",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sim.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "parallel-pioneer",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.env.reset()\n",
    "sim.env.render()\n",
    "curr_state = sim.env.reset()\n",
    "done=False\n",
    "        \n",
    "while not done:\n",
    "    sim.env.render()\n",
    "    action = sim.agent.get_action(curr_state,False)\n",
    "    s_prime, r, done, _ = sim.env.step(action)  \n",
    "    transition = {'s':curr_state,'s_prime':s_prime,'a':action,'r':r,'is_done':done}\n",
    "    curr_state = s_prime\n",
    "sim.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-sentence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harmful-force",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-tactics",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-hepatitis",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-craps",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuzzy-moses",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "political-invalid",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cubic-drunk",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-russia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-knight",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
